# Why Multiple Attempts Failed: A Debugging Post-Mortem

**Date:** November 8, 2025  
**Issue:** Image vision not working with Azure OpenAI  
**Attempts:** ~5 failed attempts before correct solution  
**Final Success:** User-guided approach based on VisionImageAdapter pattern

---

## ğŸ”´ The Problem

When uploading images, Azure OpenAI responded with:

> "I'm unable to view or analyze images directly..."

Despite implementing what seemed like correct solutions multiple times.

---

## âŒ Why My Attempts Failed

### Fundamental Misunderstanding

I made a **critical architectural mistake** from the start:

**What I Thought:**

```
Image Upload â†’ Server Storage â†’ Send URL to AI â†’ Vision Works
```

**Reality:**

```
Image Upload â†’ Server Storage â†’ Send URL to AI â†’ âŒ FAILS
Vision models need base64 data URLs, not file paths!
```

### The Failed Attempts

#### Attempt 1: Server-Side URL Conversion

```typescript
// My approach: Convert file URL to image part on server
if (part.type === "file" && isImage) {
  return {
    type: "image",
    image: `/uploads/file.jpeg`, // âŒ Wrong format!
  };
}
```

**Why it failed:** Vision models need base64 data, not file paths.

---

#### Attempt 2: Full URL Addition

```typescript
// My approach: Add http://localhost:3000 to make it a "full URL"
if (part.type === "image" && part.image.startsWith("/")) {
  return {
    type: "image",
    image: `http://localhost:3000${part.image}`, // âŒ Still wrong!
  };
}
```

**Why it failed:** Vision models can't fetch from URLs in real-time. They need inline data.

---

#### Attempt 3: Early Image Part Conversion

```typescript
// My approach: Convert file to image part immediately
if (isImage) {
  return {
    type: "image",
    image: part.url, // âŒ Lost during AI SDK conversion!
  };
}
```

**Why it failed:** AI SDK's `convertToModelMessages()` doesn't recognize custom image parts, so they were stripped out.

---

#### Attempt 4: Multiple Conversion Stages (Still Wrong Format)

```typescript
// My approach: Try to preserve through multiple stages
// But still using server paths instead of base64
```

**Why it failed:** Even with correct timing, the format was wrong (paths vs base64).

---

## ğŸ’¡ The Breakthrough: User's VisionImageAdapter Example

### What You Showed Me

```typescript
export class VisionImageAdapter implements AttachmentAdapter {
  async send(attachment: PendingAttachment): Promise<CompleteAttachment> {
    // Convert image to base64 data URL
    const base64 = await this.fileToBase64DataURL(attachment.file);

    return {
      id: attachment.id,
      type: "image",
      name: attachment.name,
      contentType: attachment.file.type,
      content: [
        {
          type: "image",
          image: base64, // â† data:image/jpeg;base64,... format
        },
      ],
      status: { type: "complete" },
    };
  }
```

### The Key Insights I Missed

1. **Client-side conversion**: Images should be converted to base64 **before** sending to server
2. **No server upload needed**: For vision, images don't need to be stored
3. **Base64 format**: `data:image/jpeg;base64,...` is the standard format
4. **Immediate availability**: No need to wait for server upload/processing

---

## ğŸ¯ Why I Kept Failing

### 1. **Wrong Mental Model**

I thought of images like documents:

- Documents â†’ Upload â†’ Extract text â†’ Send text âœ…
- Images â†’ Upload â†’ ??? â†’ Send ??? âŒ

I didn't understand that vision models work fundamentally differently.

### 2. **Didn't Know the Standard**

I didn't know that:

- Vision models expect base64 data URLs
- This is an **industry standard** (not just a quirk)
- Libraries like VisionImageAdapter exist because this is the **correct** approach

### 3. **Focused on the Wrong Layer**

I kept trying to fix things on the **server side**:

- Converting file paths to different formats
- Adding URLs
- Changing conversion timing

When the real fix needed to be on the **client side**:

- Convert to base64 immediately
- Don't upload to server at all
- Send inline data

### 4. **Didn't Test the Full Pipeline**

I was looking at **my logs** showing "image part created" âœ…  
But didn't realize the AI SDK was **stripping it out** between stages.

Your approach forced me to understand the **entire flow** from client â†’ server â†’ AI SDK â†’ Azure.

### 5. **Overcomplicating the Solution**

I was trying to make the **existing architecture** work:

- Keep server upload flow
- Try to retrofit image support
- Add conversions at multiple stages

Your approach was **simpler**:

- Images are special, handle them differently
- Client-side conversion is cleaner
- Less moving parts = fewer failure points

---

## ğŸ“š What I Learned

### Technical Lessons

1. **Vision models need base64**: Not URLs, not paths, not references
2. **Client-side is better**: For images, avoid unnecessary server round-trips
3. **Follow established patterns**: VisionImageAdapter exists for a reason
4. **Test the full pipeline**: Don't assume intermediate logs mean success
5. **AI SDK behavior matters**: Understand how `convertToModelMessages()` works

### Debugging Lessons

1. **Question your assumptions**: I assumed server upload was necessary
2. **Look for working examples**: Your VisionImageAdapter was the key
3. **Understand the standards**: Base64 for vision is industry-standard
4. **Trace data flow completely**: I missed where image parts were being lost
5. **Simpler is often better**: Client-side conversion eliminated complexity

### Communication Lessons

1. **Listen to user guidance**: You knew the right approach
2. **Don't just copy code**: Understand **why** it works
3. **Ask for examples**: Working code is worth 1000 explanations
4. **Admit when stuck**: Multiple failed attempts meant I needed help

---

## ğŸ”„ The Correct Understanding

### Before (My Wrong Model)

```
User uploads image
  â†“
Server saves to /uploads/
  â†“
Return file path
  â†“
Send path to vision model
  â†“
âŒ Model can't access file
```

### After (Correct Model from VisionImageAdapter)

```
User selects image
  â†“
Client converts to base64
  â†“
Send base64 to server (no storage needed)
  â†“
Server forwards base64 to vision model
  â†“
âœ… Model analyzes inline data
```

---

## ğŸ’­ Why This Happens to Developers

### The "Document Upload" Trap

Because **document uploads** work like this:

```
Upload â†’ Store â†’ Extract text â†’ Send text
```

We assume **all uploads** work the same way. But images are different:

```
Convert â†’ Send data â†’ Analyze
(No storage needed for vision!)
```

### The "It Should Just Work" Assumption

I thought: "The AI SDK should handle this automatically"  
Reality: "The AI SDK needs data in the correct format"

### The "Server-First" Bias

As backend developers, we often think:

```
Client â†’ Server â†’ Processing â†’ Response
```

But for images + vision:

```
Client processing â†’ Server passthrough â†’ AI
```

The **client does the heavy lifting** (base64 conversion).

---

## âœ… The Final Working Solution

### Client Side

```typescript
// Convert image to base64 immediately
const base64DataUrl = await fileToBase64DataURL(file);
return {
  url: base64DataUrl, // data:image/jpeg;base64,...
  name: file.name,
  contentType: file.type,
};
```

### Server Side

```typescript
// Stage 1: Keep as file part
if (isImage) {
  return part; // Don't convert yet
}

// Stage 2: Convert after AI SDK processing
if (part.type === "file" && part.data?.startsWith("data:image/")) {
  return {
    type: "image",
    image: part.data, // base64 data URL
  };
}
```

---

## ğŸ“ Key Takeaway

**The biggest lesson:**  
When stuck on a problem after multiple attempts, **step back and question your fundamental assumptions**.

I was solving the wrong problem (how to make file paths work) instead of the right problem (use base64 like the industry standard).

Your VisionImageAdapter example showed me:

1. What the **correct approach** looks like
2. Why my approach was **fundamentally flawed**
3. That this is a **solved problem** with established patterns

---

## ğŸ“Š Success Metrics

| Metric                         | Before | After |
| ------------------------------ | ------ | ----- |
| Failed attempts                | 5+     | 0     |
| Understanding of vision models | Poor   | Good  |
| Code complexity                | High   | Low   |
| Working vision features        | âŒ     | âœ…    |

---

## ğŸ™ Thank You

Your VisionImageAdapter example was the **critical piece** I was missing. It showed me:

- The correct architectural pattern
- The industry-standard approach
- Why my attempts were failing

Sometimes the best debugging tool is **a working example** from someone who understands the problem better.

---

**Date:** November 8, 2025  
**Lessons learned:** Don't assume, question fundamentals, follow standards  
**Result:** Working vision features âœ…
